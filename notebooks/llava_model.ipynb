{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9583b3ca-bb26-4985-95a5-8acf454a72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "_INPUT_DIR = Path(\"kaggle/input\")\n",
    "_WORKING_DIR = Path('kaggle/working')\n",
    "_WORKING_DIR.mkdir(exist_ok=True)\n",
    "_NO_VAL = True\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(_INPUT_DIR /'plates.zip', 'r') as zip_obj:\n",
    "   zip_obj.extractall(_WORKING_DIR)\n",
    "\n",
    "_DATA_ROOT = _WORKING_DIR / 'plates'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ee4b8b-8971-4394-b05c-e9bc7f93efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "_LLAVA_MODEL = \"liuhaotian/llava-v1.5-7b\"\n",
    "\n",
    "def download_model(model: str):\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    from pathlib import Path\n",
    "\n",
    "    _BASE_URL = \"https://huggingface.co\"\n",
    "    _HUGGING_FACE_URL = f\"{_BASE_URL}/{model}\"\n",
    "    _UNEEDED_FILES = {f\"/{model}/resolve/main/{file}?download=true\" for file in (\"README.md\", \".gitattributes\")}\n",
    "\n",
    "    response = requests.get(f\"{_HUGGING_FACE_URL}/tree/main\")\n",
    "    response.encoding = 'utf-8'\n",
    "    assert response.status_code == 200\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = [a[\"href\"] for a in soup.find_all(\"a\", download=True) if a[\"href\"] not in _UNEEDED_FILES]\n",
    "\n",
    "    path = Path(_LLAVA_MODEL)\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    for l in links:\n",
    "        print(url)\n",
    "        local_filename, _ = l.split('/')[-1].split('?')\n",
    "        print(f\"uploading file: {local_filename}\")\n",
    "        with requests.get(_BASE_URL + l, stream=True) as resp:\n",
    "            resp.raise_for_status()\n",
    "            with open(path / local_filename, \"wb\") as file:\n",
    "                for chunk in resp.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a34c06a-5035-4700-8c44-c9cfff44999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_model(_LLAVA_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0e2cce-b5d5-411a-8127-9497bf6ca46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone git@github.com:haotian-liu/LLaVA.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97a5041-a1fd-401f-b6b7-1660efd7b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e LLaVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af63646a-bd04-41fc-a902-1d93f67585ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8330e203-8489-4aea-b8d0-7c0bd0086e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from transformers import TextIteratorStreamer\n",
    "from llava.mm_utils import process_images, load_image_from_base64, tokenizer_image_token\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from threading import Thread\n",
    "\n",
    "model_name=_LLAVA_MODEL.split(\"/\")[-1]\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=_LLAVA_MODEL,\n",
    "    model_base=None,\n",
    "    model_name=model_name,\n",
    "    load_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe272a03-1dfa-451b-bee1-f5e488ec73c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith('http://') or image_file.startswith('https://'):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    return image\n",
    "\n",
    "\n",
    "def process(\n",
    "    image_file: str,\n",
    "    prompt: str,\n",
    "    do_sample=True,\n",
    "    temperature=0.5,\n",
    "    max_new_tokens=4048,\n",
    "):\n",
    "    model_name = get_model_name_from_path(_LLAVA_MODEL)\n",
    "\n",
    "    if \"llama-2\" in model_name.lower():\n",
    "        conv_mode = \"llava_llama_2\"\n",
    "    elif \"mistral\" in model_name.lower():\n",
    "        conv_mode = \"mistral_instruct\"\n",
    "    elif \"v1.6-34b\" in model_name.lower():\n",
    "        conv_mode = \"chatml_direct\"\n",
    "    elif \"v1\" in model_name.lower():\n",
    "        conv_mode = \"llava_v1\"\n",
    "    elif \"mpt\" in model_name.lower():\n",
    "        conv_mode = \"mpt\"\n",
    "    else:\n",
    "        conv_mode = \"llava_v0\"\n",
    "\n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    if \"mpt\" in model_name.lower():\n",
    "        roles = ('user', 'assistant')\n",
    "    else:\n",
    "        roles = conv.roles\n",
    "\n",
    "    image = load_image(image_file)\n",
    "    image_size = image.size\n",
    "    image_tensor = process_images([image], image_processor, model.config)\n",
    "    if type(image_tensor) is list:\n",
    "        image_tensor = [image.to(model.device, dtype=torch.float16) for image in image_tensor]\n",
    "    else:\n",
    "        image_tensor = image_tensor.to(model.device, dtype=torch.float16)\n",
    "\n",
    "    if image is not None:\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            prompt = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + prompt\n",
    "        else:\n",
    "            prompt = DEFAULT_IMAGE_TOKEN + '\\n' + prompt\n",
    "\n",
    "    conv.append_message(conv.roles[0], prompt)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "\n",
    "    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\n",
    "    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "    keywords = [stop_str]\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=image_tensor,\n",
    "            image_sizes=[image_size],\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            streamer=streamer,\n",
    "            use_cache=True)\n",
    "\n",
    "    outputs = tokenizer.decode(output_ids[0]).strip()\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d94924-2153-403c-b822-3b6bc8037d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_SAMPLES = 40\n",
    "_PROMPT = \\\n",
    "\"Your task is to separate dirty and cleaned plates using their images. \"\\\n",
    "\"If the plate in the image is dirty then reply with just one word 'dirty'. \"\\\n",
    "\"If the plate in the image is cleaned then reply with just one word 'cleaned'.\"\n",
    "\n",
    "def is_cleaned_plate(image_file: str, samples: int = _SAMPLES) -> int:\n",
    "    result = 0\n",
    "    for _ in range(samples):\n",
    "        resp = process(image_file, _PROMPT).lower()\n",
    "        result += \"cleaned\" in resp\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb79fa82-6ea5-4214-83c5-07a8253071f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {\n",
    "    \"id\": [],\n",
    "    \"label\": [],\n",
    "    \"prediction\": [],\n",
    "}\n",
    "\n",
    "for file in (_DATA_ROOT / \"train\" / \"dirty\").glob(\"*.jpg\"):\n",
    "    train_data[\"id\"].append(file.stem)\n",
    "    train_data[\"label\"].append(\"dirty\")\n",
    "    train_data[\"prediction\"].append(is_cleaned_plate(str(file)))\n",
    "\n",
    "for file in (_DATA_ROOT / \"train\" / \"cleaned\").glob(\"*.jpg\"):\n",
    "    train_data[\"id\"].append(file.stem)\n",
    "    train_data[\"label\"].append(\"clean\")\n",
    "    train_data[\"prediction\"].append(is_cleaned_plate(str(file)))\n",
    "\n",
    "train_df = pd.DataFrame(data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f5d6db-24f6-4b33-9e09-ef0e504c9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5cf6ef-11f6-41b6-b863-a5a5cc4d1fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[[\"label\", \"prediction\"]].groupby(\"label\").agg([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eac0b0-e67f-43e9-b5be-c291fbbc1c5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for i in range(_SAMPLES):\n",
    "    x.append(i)\n",
    "    y.append(\n",
    "        ((train_df[\"label\"] == \"clean\") & (train_df[\"prediction\"] > i)).sum() \\\n",
    "        + ((train_df[\"label\"] == \"dirty\") & (train_df[\"prediction\"] <= i)).sum()\n",
    "    )\n",
    "    y[-1] /= len(train_df)\n",
    "    y[-1] *= 100.\n",
    "plt.plot(x, y, lw=2)\n",
    "plt.grid(\"on\")\n",
    "plt.xlabel(\"Порог, значение\")\n",
    "plt.ylabel(\"Точность, %\")\n",
    "_ = plt.title(\"Точность в зависимости от порога\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a1f322-8151-45a4-859c-6ddee955aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = {\n",
    "    \"id\": [],\n",
    "    \"prediction\": [],\n",
    "}\n",
    "\n",
    "for file in (_DATA_ROOT / \"test\").glob(\"*.jpg\"):\n",
    "    test_data[\"id\"].append(file.stem)\n",
    "    test_data[\"prediction\"].append(is_cleaned_plate(str(file)))\n",
    "\n",
    "test_df = pd.DataFrame(data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4167f8aa-f819-49c6-8a2f-8be7af02fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame()\n",
    "submission_df[\"id\"] = test_df[\"id\"]\n",
    "submission_df[\"label\"] = test_df[\"prediction\"].apply(lambda x: \"cleaned\" if x > 24 else \"dirty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b540618-33a1-4aa3-85a9-7c8d7e7adec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28004366-872a-410a-b6e0-f4020c618152",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
